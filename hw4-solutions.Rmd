---
title: "POLS 503: Problem Set 4 Solutions"
author: "Jeffrey B. Arnold"
date: "May 15, 2015"
---


$$
\DeclareMathOperator{\cor}{cor}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\quantile}{quantile}
\DeclareMathOperator{\logit}{logit}
$$



# Instructions and Introduction


```{r load}
library("ggplot2")
library("dplyr")
library("broom")
library("tidyr")
source("http://pols503.github.io/pols_503_sp15/hw/hw4-functions.R")
```


# Problems {#problems}

## Showing Confidence

We will revisit the sprinters data we considered in Problem Set 2.
```{r}
sprinters <- read.csv("http://pols503.github.io/pols_503_sp15/data/sprinters.csv")
```


**a.** Estimate a linear regression of `finish` on `year` and `women`:
```{r}
mod1 <- lm(finish ~ year + women, data = sprinters)
```
We can create a plot in several ways.
Using the observed values and `augment()`:
This gets the critical value of the Student's $t$ distribution for the degrees of freedom in the regression and a 95% confidence interval (using 2 should be good enough, but I'm feeling pedantic today).
```{r}
tstar <- - qt((1 - 0.95)/ 2, df = mod1$df.residual)
```
```{r}
ggplot(mutate(augment(mod1),
              sex = ifelse(as.logical(women), "women", "men")),
              aes(x = year, y = .fitted,
                  ymax = .fitted + .se.fit * tstar,
                  ymin = .fitted - .se.fit * tstar,
                  colour = sex, fill = sex)) +
  geom_ribbon(alpha = 0.2, colour = NA) +
  geom_line() +
  geom_point(mapping = aes(y = finish))
```
Using `predict()`:
```{r}
mod1_newdata <-
  expand.grid(year = seq(min(sprinters$year), max(sprinters$year)),
                        women = c(0, 1))
mod1_yhat <-
  bind_cols(mod1_newdata, 
            as.data.frame(predict(mod1, mod1_newdata, interval = "confidence"))) %>%
  mutate(sex = ifelse(as.logical(women), "women", "men"))

ggplot() +
  geom_ribbon(data = mod1_yhat,
              mapping = aes(x = year, ymin = lwr, ymax = upr, fill = sex), alpha = 0.2) +
  geom_line(data = mod1_yhat, 
            mapping = aes(x = year, y = fit, colour = sex)) +
  geom_point(data = sprinters %>%
  mutate(sex = ifelse(as.logical(women), "women", "men")),
  mapping = aes(y = finish, x = year, colour = sex))
```


**b.**  In this problem, I'll estimate a linear regression of `finish` on `year` and `women` and the interaction thereof:
```{r}
mod2 <- lm(finish ~ year * women, data = sprinters)
```
Although the plot of predicted values can be created in several ways (see the previous), but this time I'll use augment.
```{r}
ggplot(mutate(augment(mod2),
              sex = ifelse(as.logical(women), "women", "men")),
              aes(x = year, y = .fitted,
                  ymax = .fitted + .se.fit * tstar,
                  ymin = .fitted - .se.fit * tstar,
                  colour = sex, fill = sex)) +
  geom_ribbon(alpha = 0.2, colour = NA) +
  geom_line() +
  geom_point(mapping = aes(y = finish))
```


**c.** In this problem, I'll estimate a linear regression of `finish` on `year`, `year` squared, and `women` and the interaction thereof:
```{r}
mod3 <- lm(finish ~ year * women + I(year ^ 2) * women, data = sprinters)
```
Although the plot of predicted values can be created in several ways (see the previous), but this time I'll use `augment()`.
```{r}
ggplot(mutate(augment(mod3),
              sex = ifelse(as.logical(women), "women", "men")),
              aes(x = year, y = .fitted,
                  ymax = .fitted + .se.fit * tstar,
                  ymin = .fitted - .se.fit * tstar,
                  colour = sex, fill = sex)) +
  geom_ribbon(alpha = 0.2, colour = NA) +
  geom_line() +
  geom_point(mapping = aes(y = finish))
```
    
**d.** Compare the visual fit of these models to the data within the observed period. Which do you find plausible fits?
    
    
**e.** Ummm, yes. 
```{r}
predict(mod1, newdata = expand.grid(women = c(0, 1), year = 2156))
predict(mod2, newdata = expand.grid(women = c(0, 1), year = 2156))
predict(mod3, newdata = expand.grid(women = c(0, 1), year = 2156))
```
`mod1` predicts women will be faster than men.
`mod2` predicts that in 2156 the times of men and women will be equal.
`mod3` predicts that in 2156 women will be 4 seconds faster than men, and even slower in 2001. 
    
**f.** Now create a new variable, the ratio of men’s time to women’s time in each
   year.
   Logit-transform this variable and regress it on year. Plot the results,with confidence intervals, on the scale of the ratio men’s time to women’s time (i.e., transform it back from logit).
   Does this approach make any assumptions about men’s times or women’s times that might be problematic?
   
```{r}
sprinters2 <- 
  sprinters %>%
  mutate(sex = ifelse(as.logical(women), "women", "men")) %>%
  select(-women) %>%
  spread(sex, finish) %>%
  mutate(rat = men / women,
         logit_rat = log(rat / (1 - rat))) %>%
  na.omit()

logit <- function(x) log(x / (1 - x))
inv_logit <- function(x) exp(x) / (exp(x) + 1)

mod4 <- lm(logit(rat) ~ year, data = sprinters2)

ggplot(augment(mod4),
       aes(x = year,
           y = inv_logit(.fitted),
           ymax = inv_logit(.fitted + .se.fit * 2),
           ymin = inv_logit(.fitted - .se.fit * 2))) +
  geom_ribbon(alpha = 0.2, colour = NA) +
  geom_line() +
  geom_point(mapping = aes(y = inv_logit(logit.rat.))) +
  scale_y_continuous("logit(men finish / women finish)")

```


## Model Selection: Oil & Democracy


```{r}
ross95 <- read.csv("http://pols503.github.io/pols_503_sp15/data/rossoildata.csv") %>%
   group_by(id) %>%
   mutate(oilL5 = lag(oil, 5),
          metalL5 = lag(metal, 5),
          GDPcapL5 = lag(GDPcap, 5)) %>%
   filter(year == 1995) %>%
   select(regime1, oilL5, metalL5, GDPcapL5, islam, oecd, cty_name, id, id1) %>%
   na.omit()
```


**a.** 

```{r}
oil_lm_1 <- lm(regime1 ~ oilL5 + metalL5 + GDPcapL5 + islam + oecd, data = ross95)
```

The standard error of the regression is 
```{r}
glance(oil_lm_1)$sigma
```

The change in `regime1` associated with a change from the 50th percentile to the 95th percentile of oil, holding all other variables at their means (including binary) is:
```{r}
oil_lm_1_oil_predict_lo <- 
  predict(oil_lm_1,
          newdata = summarise(ross95, 
                              oilL5 = median(oilL5),
                              metalL5 = mean(metalL5),
                              islam = mean(islam),
                              oecd = mean(oecd),
                              GDPcapL5 = mean(GDPcapL5)))

oil_lm_1_oil_predict_hi <- 
  predict(oil_lm_1,
          newdata = summarise(ross95, 
                              oilL5 = quantile(oilL5, 0.95),
                              metalL5 = mean(metalL5),
                              islam = mean(islam),
                              oecd = mean(oecd),
                              GDPcapL5 = mean(GDPcapL5)))

oil_lm_1_oil_predict_hi - 
  oil_lm_1_oil_predict_lo
```

**b.** Using the residuals from the regression in part a., create the following diagnostic plots:
    
    1. Plot the residuals against the fitted values
    2. Plot the residuals against each covariate
    3. Plot the studentized residuals against the standardized hat values.
    4. Calculate the heteroskedastic consistent standard errors and compare to the classical standard errors. Use the **car** function `hccm`.
    
    What do these diagnostics tell you about the presence of heteroskedasticity, specification error, and outliers?
    
c. Rerun the regression using either log or logit transformations on any covariates you see fit.
    You will likely run several specifications.
    In each run, record the standard error of the regression, and the expected change in `regime1` given a change in `oilL5` from the 50th percentile to the 95th percentile of the fully observed data.
    See the appendix for some tips and warnings about transforming these data, though.
f. How much substantive difference does finding the best model make?
    Be specific and concrete; i.e., show what each model does.
    I’m asking for a more detailed answer than you usually see in articles.
    How much substantive doubt is there in the result if we are not sure which of the models you fit is the  "right" one?
g. Which model of those you have estimated do you trust most, and why?
    What other problems in the specification or estimation method remain unaddressed by our efforts?
    
# Appendix:  How Do I Log a Covariate with Zeros?

*Christopher Adolph*

If you try to log or logit transform a covariate $x$ with observed
zeros, you will discover a problem: you can't log a zero!
A common (but wrong) "solution" is to add a small amount to the zeros
(e.g., 0.1 or 0.001, etc.).
It turns out that you can introduce substantial large bias in your $\hat \beta$s by choosing different tiny amounts to add to your 0s: logging small numbers spreads those numbers out over a huge range.
Adding 0.001 before logging a variable is not very different from subtracting 10,000 from an unlogged variable!
So don't ever do this, even as a first try.

## A Solution: the logBound and logitBound Transformations

A better solution that avoids arbitrary assumptions and bias is to
"dummy out" the zeros before logging. 
This procedure treats the zero cases as \emph{sui
generis}:  they are uniquely different from the rest of our cases,
and we estimate the way in which they are different through a
separate parameter.  We end up with two variables on the right-hand
side: an indicator of whether $x_i=0$, and the log (or logit) of
$x_i$ in those cases where $x_i \ne 0$.  That is, if you want to
regress $y$ on $\textrm{log}(x)$ but $x$ contains $0$s, estimate this
regression:
$$
y_i = \beta_0 + \beta_1 I(x_i>0) + \beta_2 \log'(x_i) + \epsilon_i
$$
where $I(\cdot)$ is an indicator function and $\log'(\cdot)$ is defined as:
$$
\log'(x) =
\begin{cases}
0 &\text{if $\quad x \le 0$} \\
\log(x) & \text{if $x > 0$} 
\end{cases}
$$

If we suppose that $x_i$ is the number of cigarettes person $i$ smokes
per day, and $y_i$ is $i$'s probability of getting lung cancer, the
specification makes sense:  people who currently smoke even a little bit
likely have a discretely higher chance of lung cancer than
non-smokers, while the amount a smoker smokes may increase cancer
probabilities but with diminishing marginal risk.

As you might imagine, the logic of equations 1 and 2
changes slightly if $x$ needs to be logit transformed.  Recall that the logit
transformation,
$$
\logit(x) = \log\left( \frac{x}{1-x} \right),
$$
\noindent fails if $x$ is not between 0 and 1, so we need to dummy out $x_i \ge 1$ and $x_i \le 0$ separately:
$$
y_i = \beta_0 + \beta_1 I(x_i>0) + \beta_2 I(x_i \ge 1) + \beta_3 \logit'(x_i) + \epsilon_i
$$
where $I(\cdot)$ is an indicator function and $\logit'(\cdot)$ is defined as:
$$
\logit'(x) =
\begin{cases}
0 &\mathrm{if} \quad x \le 0 \\
\log \left( \frac{x}{1-x} \right)&
    \mathrm{if} \quad 0<x<1 \\ 
0 &\mathrm{if} \quad x \ge 1
\end{cases}
$$
Note that we will only need all three pieces of Equation 4 if the covariate to be logit transformed contains **both** 0s and 1s; should either extreme be missing, we need only add one dummy variable to the specification.

The `logBound()` and `logitBound()` functions are included in `hw4-functions.R`.
You can load these functions with the following code (provided you downloaded `hw4-functions.R` and put it in your package directory):
```{r}
source("hw4-functions.R")
```
After loading these functions, the above regression is as simple as:
```{r eval=FALSE}
res <- lm(y ~ logBound(x), data)
```
You can compare goodness of fit as usual.
Moreover, you can use this technique on the right-hand side of any regression-like model, not just least squares regression.

## Interpretation of results

Take special care in interpreting models in models with `logBound(x)` or `logitBound(x)` in the model formula.
In setting up a hypothetical scenario for post-estimation prediction, make sure both the dummy term and the log term are set consistent with each other.
For example, if the dummy is set to 0, the log must also be zero.
And if the log is set to something other than 0, the dummy must be set to 1.
Otherwise, you are asking the model to predict a logically impossible scenario; e.g., asking what happens when someone both smokes zero cigarettes and smokes twenty cigarettes in the same day.

I recommend either calculating the predicted values of `regime1` "by hand", or using the **simcf** package, as illustrated below.
Our old friend `predict()` is very unlikely to return results for models including these terms, though if it does return an answer it will agree with other methods.

* * *

Derived from of Christopher Adolph, "Problem Set 4", *POLS/CSSS 503*, University of Washington, Spring 2014. <http://faculty.washington.edu/cadolph/503/503hw4.pdf>. Used with permission.

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
